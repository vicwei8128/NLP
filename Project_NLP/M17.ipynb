{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install tensorflow-addons`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import datetime\n",
    "import tensorflow_addons as tfa\n",
    "PAD_ID = 0\n",
    "\n",
    "class DateData:\n",
    "    def __init__(self, n):\n",
    "        np.random.seed(1)\n",
    "        self.date_cn = []\n",
    "        self.date_en = []\n",
    "        for timestamp in np.random.randint(143835585, 2043835585, n):\n",
    "            date = datetime.datetime.fromtimestamp(timestamp)\n",
    "            self.date_cn.append(date.strftime(\"%y-%m-%d\"))\n",
    "            self.date_en.append(date.strftime(\"%d/%b/%Y\"))\n",
    "        self.vocab = set(\n",
    "            [str(i) for i in range(0, 10)] + [\"-\", \"/\", \"<GO>\", \"<EOS>\"] + [\n",
    "                i.split(\"/\")[1] for i in self.date_en])\n",
    "        self.v2i = {v: i for i, v in enumerate(sorted(list(self.vocab)), start=1)}\n",
    "        self.v2i[\"<PAD>\"] = PAD_ID\n",
    "        self.vocab.add(\"<PAD>\")\n",
    "        self.i2v = {i: v for v, i in self.v2i.items()}\n",
    "        self.x, self.y = [], []\n",
    "        for cn, en in zip(self.date_cn, self.date_en):\n",
    "            self.x.append([self.v2i[v] for v in cn])\n",
    "            self.y.append(\n",
    "                [self.v2i[\"<GO>\"], ] + [self.v2i[v] for v in en[:3]] + [\n",
    "                    self.v2i[en[3:6]], ] + [self.v2i[v] for v in en[6:]] + [\n",
    "                    self.v2i[\"<EOS>\"], ])\n",
    "        self.x, self.y = np.array(self.x), np.array(self.y)\n",
    "        self.start_token = self.v2i[\"<GO>\"]\n",
    "        self.end_token = self.v2i[\"<EOS>\"]\n",
    "\n",
    "    def sample(self, n=64):\n",
    "        bi = np.random.randint(0, len(self.x), size=n)\n",
    "        bx, by = self.x[bi], self.y[bi]\n",
    "        decoder_len = np.full((len(bx),), by.shape[1] - 1, dtype=np.int32)\n",
    "        return bx, by, decoder_len\n",
    "\n",
    "    def idx2str(self, idx):\n",
    "        x = []\n",
    "        for i in idx:\n",
    "            x.append(self.i2v[i])\n",
    "            if i == self.end_token:\n",
    "                break\n",
    "        return \"\".join(x)\n",
    "\n",
    "    @property\n",
    "    def num_word(self):\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(keras.Model):\n",
    "    def __init__(self, enc_v_dim, dec_v_dim, emb_dim, units, max_pred_len, start_token, end_token):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "        # encoder\n",
    "        self.enc_embeddings = keras.layers.Embedding(\n",
    "            input_dim=enc_v_dim, output_dim=emb_dim,  # [enc_n_vocab, emb_dim]\n",
    "            embeddings_initializer=tf.initializers.RandomNormal(0., 0.1),\n",
    "        )\n",
    "        self.encoder = keras.layers.LSTM(units=units, return_sequences=True, return_state=True)\n",
    "\n",
    "        # decoder\n",
    "        self.dec_embeddings = keras.layers.Embedding(\n",
    "            input_dim=dec_v_dim, output_dim=emb_dim,  # [dec_n_vocab, emb_dim]\n",
    "            embeddings_initializer=tf.initializers.RandomNormal(0., 0.1),\n",
    "        )\n",
    "        self.decoder_cell = keras.layers.LSTMCell(units=units)\n",
    "        decoder_dense = keras.layers.Dense(dec_v_dim)\n",
    "        # train decoder\n",
    "        self.decoder_train = tfa.seq2seq.BasicDecoder(\n",
    "            cell=self.decoder_cell,\n",
    "            sampler=tfa.seq2seq.sampler.TrainingSampler(),   # sampler for train\n",
    "            output_layer=decoder_dense\n",
    "        )\n",
    "        # predict decoder\n",
    "        self.decoder_eval = tfa.seq2seq.BasicDecoder(\n",
    "            cell=self.decoder_cell,\n",
    "            sampler=tfa.seq2seq.sampler.GreedyEmbeddingSampler(),       # sampler for predict\n",
    "            output_layer=decoder_dense\n",
    "        )\n",
    "\n",
    "        self.cross_entropy = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        self.opt = keras.optimizers.Adam(0.01)\n",
    "        self.max_pred_len = max_pred_len\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "\n",
    "    def encode(self, x):\n",
    "        embedded = self.enc_embeddings(x)\n",
    "        init_s = [tf.zeros((x.shape[0], self.units)), tf.zeros((x.shape[0], self.units))]\n",
    "        o, h, c = self.encoder(embedded, initial_state=init_s)\n",
    "        return [h, c]\n",
    "\n",
    "    def inference(self, x):\n",
    "        s = self.encode(x)\n",
    "        done, i, s = self.decoder_eval.initialize(\n",
    "            self.dec_embeddings.variables[0],\n",
    "            start_tokens=tf.fill([x.shape[0], ], self.start_token),\n",
    "            end_token=self.end_token,\n",
    "            initial_state=s,\n",
    "        )\n",
    "        pred_id = np.zeros((x.shape[0], self.max_pred_len), dtype=np.int32)\n",
    "        for l in range(self.max_pred_len):\n",
    "            o, s, i, done = self.decoder_eval.step(\n",
    "                time=l, inputs=i, state=s, training=False)\n",
    "            pred_id[:, l] = o.sample_id\n",
    "        return pred_id\n",
    "\n",
    "    def train_logits(self, x, y, seq_len):\n",
    "        s = self.encode(x)\n",
    "        dec_in = y[:, :-1]   # ignore <EOS>\n",
    "        dec_emb_in = self.dec_embeddings(dec_in)\n",
    "        o, _, _ = self.decoder_train(dec_emb_in, s, sequence_length=seq_len)\n",
    "        logits = o.rnn_output\n",
    "        return logits\n",
    "\n",
    "    def step(self, x, y, seq_len):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.train_logits(x, y, seq_len)\n",
    "            dec_out = y[:, 1:]  # ignore <GO>\n",
    "            loss = self.cross_entropy(dec_out, logits)\n",
    "            grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese time order: yy/mm/dd  ['31-04-26', '04-07-18', '33-06-06'] \n",
      "English time order: dd/M/yyyy  ['26/Apr/2031', '18/Jul/2004', '06/Jun/2033']\n",
      "vocabularies:  {'<EOS>', 'Aug', 'Dec', '5', '2', 'Apr', '-', '1', 'Jul', '<GO>', 'Jun', '/', '9', '7', 'Nov', '4', '6', 'Sep', '8', '<PAD>', 'May', '3', '0', 'Mar', 'Jan', 'Feb', 'Oct'}\n",
      "x index sample: \n",
      "31-04-26\n",
      "[6 4 1 3 7 1 5 9] \n",
      "y index sample: \n",
      "<GO>26/Apr/2031<EOS>\n",
      "[14  5  9  2 15  2  5  3  6  4 13]\n"
     ]
    }
   ],
   "source": [
    "data = DateData(4000)\n",
    "print(\"Chinese time order: yy/mm/dd \", data.date_cn[:3], \"\\nEnglish time order: dd/M/yyyy \", data.date_en[:3])\n",
    "print(\"vocabularies: \", data.vocab)\n",
    "print(\"x index sample: \\n{}\\n{}\".format(data.idx2str(data.x[0]), data.x[0]),\n",
    "      \"\\ny index sample: \\n{}\\n{}\".format(data.idx2str(data.y[0]), data.y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:  0 | loss: 3.302 | input:  96-06-17 | target:  17/Jun/1996 | inference:  //Jul/0/00/0/\n",
      "t:  70 | loss: 1.096 | input:  91-08-19 | target:  19/Aug/1991 | inference:  16/Jul/2001<EOS>\n",
      "t:  140 | loss: 0.768 | input:  11-04-30 | target:  30/Apr/2011 | inference:  03/May/2023<EOS>\n",
      "t:  210 | loss: 0.511 | input:  76-03-14 | target:  14/Mar/1976 | inference:  14/May/1985<EOS>\n",
      "t:  280 | loss: 0.370 | input:  83-11-19 | target:  19/Nov/1983 | inference:  19/Jan/1984<EOS>\n",
      "t:  350 | loss: 0.223 | input:  08-07-22 | target:  22/Jul/2008 | inference:  22/Mar/2008<EOS>\n",
      "t:  420 | loss: 0.151 | input:  05-02-10 | target:  10/Feb/2005 | inference:  10/Aug/2005<EOS>\n",
      "t:  490 | loss: 0.103 | input:  32-10-02 | target:  02/Oct/2032 | inference:  02/Jan/2032<EOS>\n",
      "t:  560 | loss: 0.067 | input:  81-06-29 | target:  29/Jun/1981 | inference:  29/Jun/1981<EOS>\n",
      "t:  630 | loss: 0.032 | input:  76-08-18 | target:  18/Aug/1976 | inference:  18/Aug/1976<EOS>\n",
      "t:  700 | loss: 0.016 | input:  87-02-16 | target:  16/Feb/1987 | inference:  16/Feb/1987<EOS>\n",
      "t:  770 | loss: 0.017 | input:  78-12-10 | target:  10/Dec/1978 | inference:  10/Dec/1978<EOS>\n",
      "t:  840 | loss: 0.011 | input:  80-01-16 | target:  16/Jan/1980 | inference:  16/Jan/1980<EOS>\n",
      "t:  910 | loss: 0.008 | input:  29-10-12 | target:  12/Oct/2029 | inference:  12/Oct/2029<EOS>\n",
      "t:  980 | loss: 0.006 | input:  79-06-17 | target:  17/Jun/1979 | inference:  17/Jun/1979<EOS>\n",
      "t:  1050 | loss: 0.005 | input:  02-12-23 | target:  23/Dec/2002 | inference:  23/Dec/2002<EOS>\n",
      "t:  1120 | loss: 0.005 | input:  02-05-15 | target:  15/May/2002 | inference:  15/May/2002<EOS>\n",
      "t:  1190 | loss: 0.004 | input:  15-12-30 | target:  30/Dec/2015 | inference:  30/Dec/2015<EOS>\n",
      "t:  1260 | loss: 0.061 | input:  14-11-29 | target:  29/Nov/2014 | inference:  29/Nov/2014<EOS>\n",
      "t:  1330 | loss: 0.006 | input:  89-09-25 | target:  25/Sep/1989 | inference:  25/Sep/1989<EOS>\n",
      "t:  1400 | loss: 0.004 | input:  86-10-14 | target:  14/Oct/1986 | inference:  14/Oct/1986<EOS>\n",
      "t:  1470 | loss: 0.003 | input:  18-02-08 | target:  08/Feb/2018 | inference:  08/Feb/2018<EOS>\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq(\n",
    "    data.num_word, data.num_word, emb_dim=16, units=32,\n",
    "    max_pred_len=11, start_token=data.start_token, end_token=data.end_token)\n",
    "\n",
    "# training\n",
    "for t in range(1500):\n",
    "    bx, by, decoder_len = data.sample(32)\n",
    "    loss = model.step(bx, by, decoder_len)\n",
    "    if t % 70 == 0:\n",
    "        target = data.idx2str(by[0, 1:-1])\n",
    "        pred = model.inference(bx[0:1])\n",
    "        res = data.idx2str(pred[0])\n",
    "        src = data.idx2str(bx[0])\n",
    "        print(\n",
    "            \"t: \", t,\n",
    "            \"| loss: %.3f\" % loss,\n",
    "            \"| input: \", src,\n",
    "            \"| target: \", target,\n",
    "            \"| inference: \", res,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import datetime\n",
    "import tensorflow_addons as tfa\n",
    "PAD_ID = 0\n",
    "\n",
    "class DateData:\n",
    "    def __init__(self, n):\n",
    "        np.random.seed(1)\n",
    "        self.date_cn = []\n",
    "        self.date_en = []\n",
    "        for timestamp in np.random.randint(143835585, 2043835585, n):\n",
    "            date = datetime.datetime.fromtimestamp(timestamp)\n",
    "            self.date_cn.append(date.strftime(\"%y-%m-%d\"))\n",
    "            self.date_en.append(date.strftime(\"%d/%b/%Y\"))\n",
    "        self.vocab = set(\n",
    "            [str(i) for i in range(0, 10)] + [\"-\", \"/\", \"<GO>\", \"<EOS>\"] + [\n",
    "                i.split(\"/\")[1] for i in self.date_en])\n",
    "        self.v2i = {v: i for i, v in enumerate(sorted(list(self.vocab)), start=1)}\n",
    "        self.v2i[\"<PAD>\"] = PAD_ID\n",
    "        self.vocab.add(\"<PAD>\")\n",
    "        self.i2v = {i: v for v, i in self.v2i.items()}\n",
    "        self.x, self.y = [], []\n",
    "        for cn, en in zip(self.date_cn, self.date_en):\n",
    "            self.x.append([self.v2i[v] for v in cn])\n",
    "            self.y.append(\n",
    "                [self.v2i[\"<GO>\"], ] + [self.v2i[v] for v in en[:3]] + [\n",
    "                    self.v2i[en[3:6]], ] + [self.v2i[v] for v in en[6:]] + [\n",
    "                    self.v2i[\"<EOS>\"], ])\n",
    "        self.x, self.y = np.array(self.x), np.array(self.y)\n",
    "        self.start_token = self.v2i[\"<GO>\"]\n",
    "        self.end_token = self.v2i[\"<EOS>\"]\n",
    "\n",
    "    def sample(self, n=64):\n",
    "        bi = np.random.randint(0, len(self.x), size=n)\n",
    "        bx, by = self.x[bi], self.y[bi]\n",
    "        decoder_len = np.full((len(bx),), by.shape[1] - 1, dtype=np.int32)\n",
    "        return bx, by, decoder_len\n",
    "\n",
    "    def idx2str(self, idx):\n",
    "        x = []\n",
    "        for i in idx:\n",
    "            x.append(self.i2v[i])\n",
    "            if i == self.end_token:\n",
    "                break\n",
    "        return \"\".join(x)\n",
    "\n",
    "    @property\n",
    "    def num_word(self):\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow_addons as tfa\n",
    "import pickle\n",
    "\n",
    "\n",
    "class Seq2Seq(keras.Model):\n",
    "    def __init__(self, enc_v_dim, dec_v_dim, emb_dim, units, attention_layer_size, max_pred_len, start_token, end_token):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "        # encoder\n",
    "        self.enc_embeddings = keras.layers.Embedding(\n",
    "            input_dim=enc_v_dim, output_dim=emb_dim,    # [enc_n_vocab, emb_dim]\n",
    "            embeddings_initializer=tf.initializers.RandomNormal(0., 0.1),\n",
    "        )\n",
    "        self.encoder = keras.layers.LSTM(units=units, return_sequences=True, return_state=True)\n",
    "\n",
    "        # decoder\n",
    "        self.attention = tfa.seq2seq.LuongAttention(units, memory=None, memory_sequence_length=None)\n",
    "        self.decoder_cell = tfa.seq2seq.AttentionWrapper(\n",
    "            cell=keras.layers.LSTMCell(units=units),\n",
    "            attention_mechanism=self.attention,\n",
    "            attention_layer_size=attention_layer_size,\n",
    "            alignment_history=True,                     # for attention visualization\n",
    "        )\n",
    "\n",
    "        self.dec_embeddings = keras.layers.Embedding(\n",
    "            input_dim=dec_v_dim, output_dim=emb_dim,    # [dec_n_vocab, emb_dim]\n",
    "            embeddings_initializer=tf.initializers.RandomNormal(0., 0.1),\n",
    "        )\n",
    "        decoder_dense = keras.layers.Dense(dec_v_dim)   # output layer\n",
    "\n",
    "        # train decoder\n",
    "        self.decoder_train = tfa.seq2seq.BasicDecoder(\n",
    "            cell=self.decoder_cell,\n",
    "            sampler=tfa.seq2seq.sampler.TrainingSampler(),   # sampler for train\n",
    "            output_layer=decoder_dense\n",
    "        )\n",
    "        self.cross_entropy = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        self.opt = keras.optimizers.Adam(0.05, clipnorm=5.0)\n",
    "\n",
    "        # predict decoder\n",
    "        self.decoder_eval = tfa.seq2seq.BasicDecoder(\n",
    "            cell=self.decoder_cell,\n",
    "            sampler=tfa.seq2seq.sampler.GreedyEmbeddingSampler(),       # sampler for predict\n",
    "            output_layer=decoder_dense\n",
    "        )\n",
    "\n",
    "        # prediction restriction\n",
    "        self.max_pred_len = max_pred_len\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "\n",
    "    def encode(self, x):\n",
    "        o = self.enc_embeddings(x)\n",
    "        init_s = [tf.zeros((x.shape[0], self.units)), tf.zeros((x.shape[0], self.units))]\n",
    "        o, h, c = self.encoder(o, initial_state=init_s)\n",
    "        return o, h, c\n",
    "\n",
    "    def set_attention(self, x):\n",
    "        o, h, c = self.encode(x)\n",
    "        # encoder output for attention to focus\n",
    "        self.attention.setup_memory(o)\n",
    "        # wrap state by attention wrapper\n",
    "        s = self.decoder_cell.get_initial_state(batch_size=x.shape[0], dtype=tf.float32).clone(cell_state=[h, c])\n",
    "        return s\n",
    "\n",
    "    def inference(self, x, return_align=False):\n",
    "        s = self.set_attention(x)\n",
    "        done, i, s = self.decoder_eval.initialize(\n",
    "            self.dec_embeddings.variables[0],\n",
    "            start_tokens=tf.fill([x.shape[0], ], self.start_token),\n",
    "            end_token=self.end_token,\n",
    "            initial_state=s,\n",
    "        )\n",
    "        pred_id = np.zeros((x.shape[0], self.max_pred_len), dtype=np.int32)\n",
    "        for l in range(self.max_pred_len):\n",
    "            o, s, i, done = self.decoder_eval.step(\n",
    "                time=l, inputs=i, state=s, training=False)\n",
    "            pred_id[:, l] = o.sample_id\n",
    "        if return_align:\n",
    "            return np.transpose(s.alignment_history.stack().numpy(), (1, 0, 2))\n",
    "        else:\n",
    "            s.alignment_history.mark_used()  # otherwise gives warning\n",
    "            return pred_id\n",
    "\n",
    "    def train_logits(self, x, y, seq_len):\n",
    "        s = self.set_attention(x)\n",
    "        dec_in = y[:, :-1]   # ignore <EOS>\n",
    "        dec_emb_in = self.dec_embeddings(dec_in)\n",
    "        o, _, _ = self.decoder_train(dec_emb_in, s, sequence_length=seq_len)\n",
    "        logits = o.rnn_output\n",
    "        return logits\n",
    "\n",
    "    def step(self, x, y, seq_len):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.train_logits(x, y, seq_len)\n",
    "            dec_out = y[:, 1:]  # ignore <GO>\n",
    "            loss = self.cross_entropy(dec_out, logits)\n",
    "            grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese time order: yy/mm/dd  ['31-04-26', '04-07-18', '33-06-06'] \n",
      "English time order: dd/M/yyyy  ['26/Apr/2031', '18/Jul/2004', '06/Jun/2033']\n",
      "vocabularies:  {'7', '2', 'Apr', '1', 'Sep', 'Mar', '4', '<GO>', '9', 'Feb', 'Oct', 'Nov', 'Jun', '<PAD>', 'Jul', 'Dec', '<EOS>', '8', '3', 'May', '-', '5', 'Jan', '0', '6', 'Aug', '/'}\n",
      "x index sample: \n",
      "31-04-26\n",
      "[6 4 1 3 7 1 5 9] \n",
      "y index sample: \n",
      "<GO>26/Apr/2031<EOS>\n",
      "[14  5  9  2 15  2  5  3  6  4 13]\n",
      "t:  0 | loss: 3.29482 | input:  89-05-25 | target:  25/May/1989 | inference:  22222000000\n",
      "t:  70 | loss: 0.50147 | input:  03-09-13 | target:  13/Sep/2003 | inference:  13/Sep/2000<EOS>\n",
      "t:  140 | loss: 0.11515 | input:  92-06-01 | target:  01/Jun/1992 | inference:  01/Jan/1992<EOS>\n",
      "t:  210 | loss: 0.00174 | input:  23-01-28 | target:  28/Jan/2023 | inference:  28/Jan/2023<EOS>\n",
      "t:  280 | loss: 0.00048 | input:  25-08-01 | target:  01/Aug/2025 | inference:  01/Aug/2025<EOS>\n",
      "t:  350 | loss: 0.00032 | input:  75-04-21 | target:  21/Apr/1975 | inference:  21/Apr/1975<EOS>\n",
      "t:  420 | loss: 0.00024 | input:  13-06-05 | target:  05/Jun/2013 | inference:  05/Jun/2013<EOS>\n",
      "t:  490 | loss: 0.00021 | input:  06-10-16 | target:  16/Oct/2006 | inference:  16/Oct/2006<EOS>\n",
      "t:  560 | loss: 0.00015 | input:  08-05-29 | target:  29/May/2008 | inference:  29/May/2008<EOS>\n",
      "t:  630 | loss: 0.00013 | input:  78-02-16 | target:  16/Feb/1978 | inference:  16/Feb/1978<EOS>\n",
      "t:  700 | loss: 0.00010 | input:  34-08-04 | target:  04/Aug/2034 | inference:  04/Aug/2034<EOS>\n",
      "t:  770 | loss: 0.00008 | input:  75-04-10 | target:  10/Apr/1975 | inference:  10/Apr/1975<EOS>\n",
      "t:  840 | loss: 0.00008 | input:  86-06-28 | target:  28/Jun/1986 | inference:  28/Jun/1986<EOS>\n",
      "t:  910 | loss: 0.00005 | input:  11-09-13 | target:  13/Sep/2011 | inference:  13/Sep/2011<EOS>\n",
      "t:  980 | loss: 0.00006 | input:  06-08-10 | target:  10/Aug/2006 | inference:  10/Aug/2006<EOS>\n"
     ]
    }
   ],
   "source": [
    "data = DateData(2000)\n",
    "print(\"Chinese time order: yy/mm/dd \", data.date_cn[:3], \"\\nEnglish time order: dd/M/yyyy \", data.date_en[:3])\n",
    "print(\"vocabularies: \", data.vocab)\n",
    "print(\"x index sample: \\n{}\\n{}\".format(data.idx2str(data.x[0]), data.x[0]),\n",
    "        \"\\ny index sample: \\n{}\\n{}\".format(data.idx2str(data.y[0]), data.y[0]))\n",
    "\n",
    "model = Seq2Seq(\n",
    "    data.num_word, data.num_word, emb_dim=12, units=14, attention_layer_size=16,\n",
    "    max_pred_len=11, start_token=data.start_token, end_token=data.end_token)\n",
    "\n",
    "# training\n",
    "for t in range(1000):\n",
    "    bx, by, decoder_len = data.sample(64)\n",
    "    loss = model.step(bx, by, decoder_len)\n",
    "    if t % 70 == 0:\n",
    "        target = data.idx2str(by[0, 1:-1])\n",
    "        pred = model.inference(bx[0:1])\n",
    "        res = data.idx2str(pred[0])\n",
    "        src = data.idx2str(bx[0])\n",
    "        print(\n",
    "            \"t: \", t,\n",
    "            \"| loss: %.5f\" % loss,\n",
    "            \"| input: \", src,\n",
    "            \"| target: \", target,\n",
    "            \"| inference: \", res,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
